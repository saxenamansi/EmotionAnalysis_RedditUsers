{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8922e5-8d3c-40d1-9fed-588fc9f0e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e9a195-c5e3-4e94-8edf-6cf09986b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first GPU\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "    print(\"using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6fae2-c1cc-4034-9e15-ee307d18006b",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f565a78-443b-4a14-8591-bbac5630c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile all reddit posts together\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "datapath = \"data/userReddit.csv\"\n",
    "reddit_data = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec955e9f-72ab-4711-a9ed-0b5fcfb637b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>userId</th>\n",
       "      <th>createdTime</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi I'm Rick Astley. Good to be back here again...</td>\n",
       "      <td>ReallyRickAstley</td>\n",
       "      <td>1.697728e+09</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.806, 'pos': 0.194, 'comp...</td>\n",
       "      <td>hi im rick astley good back my new album are w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Live stream chat with my new album and upcomin...</td>\n",
       "      <td>ReallyRickAstley</td>\n",
       "      <td>1.697647e+09</td>\n",
       "      <td>{'neg': 0.06, 'neu': 0.713, 'pos': 0.227, 'com...</td>\n",
       "      <td>live stream chat new album upcoming ama hi im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coming soon…\\n</td>\n",
       "      <td>ReallyRickAstley</td>\n",
       "      <td>1.697149e+09</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>coming soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rick Astley (me) - Never Gonna Stop [Pop]\\n</td>\n",
       "      <td>ReallyRickAstley</td>\n",
       "      <td>1.693502e+09</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.726, 'pos': 0.274, 'comp...</td>\n",
       "      <td>rick astley never gonna stop pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rick Astley (me) - Dippin My Feet [Pop]\\n</td>\n",
       "      <td>ReallyRickAstley</td>\n",
       "      <td>1.689269e+09</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>rick astley dippin my feet pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content            userId  \\\n",
       "0  Hi I'm Rick Astley. Good to be back here again...  ReallyRickAstley   \n",
       "1  Live stream chat with my new album and upcomin...  ReallyRickAstley   \n",
       "2                                     Coming soon…\\n  ReallyRickAstley   \n",
       "3        Rick Astley (me) - Never Gonna Stop [Pop]\\n  ReallyRickAstley   \n",
       "4          Rick Astley (me) - Dippin My Feet [Pop]\\n  ReallyRickAstley   \n",
       "\n",
       "    createdTime                                          sentiment  \\\n",
       "0  1.697728e+09  {'neg': 0.0, 'neu': 0.806, 'pos': 0.194, 'comp...   \n",
       "1  1.697647e+09  {'neg': 0.06, 'neu': 0.713, 'pos': 0.227, 'com...   \n",
       "2  1.697149e+09  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3  1.693502e+09  {'neg': 0.0, 'neu': 0.726, 'pos': 0.274, 'comp...   \n",
       "4  1.689269e+09  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "                                preprocessed_content  \n",
       "0  hi im rick astley good back my new album are w...  \n",
       "1  live stream chat new album upcoming ama hi im ...  \n",
       "2                                        coming soon  \n",
       "3                   rick astley never gonna stop pop  \n",
       "4                     rick astley dippin my feet pop  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aae1b22-335e-418c-b412-1864c93597f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5523, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c85864-51e5-4b06-bc56-2bbdbb14f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hi im rick astley good back my new album are w...\n",
       "1    live stream chat new album upcoming ama hi im ...\n",
       "2                                          coming soon\n",
       "3                     rick astley never gonna stop pop\n",
       "4                       rick astley dippin my feet pop\n",
       "5                                    john never giving\n",
       "6    its great run ive decided devote next ten year...\n",
       "7                             ready waiting james gunn\n",
       "8    rick astley either way chris stapleton cover f...\n",
       "9    never gonna give you up 35 years old today giv...\n",
       "Name: preprocessed_content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = reddit_data['preprocessed_content']\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f9cb81-272c-4b8d-9f75-9ddb9ca9c315",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "a. SamLowe/roberta-base-go_emotions \\\n",
    "b. joeddav/distilbert-base-uncased-go-emotions-student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c18cfe-e971-468b-bb4a-bd06e02b1cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 16:47:16.622177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 16:47:17.163849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# SamLowe/roberta-base-go_emotions\n",
    "from transformers import pipeline\n",
    "\n",
    "rb_classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a17fc8-98e4-4ef9-ac7d-baa925ae5ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6638 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    " # i in tqdm.trange(0, len(a1_conversations)):\n",
    "\n",
    "rb_outputs = []\n",
    "rb_errors = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "  try:\n",
    "    rb_output = rb_classifier(sent)\n",
    "    rb_outputs.append(rb_output[0])\n",
    "  except:\n",
    "    rb_errors.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0c7699-a62d-45ff-859e-bef0b373bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the JSON object to a file\n",
    "with open('User_Reddit_Classification_Results/rb_outputs.json', 'w') as outfile:\n",
    "    json.dump(rb_outputs, outfile, indent=4)\n",
    "\n",
    "# Serialize the list to a JSON file\n",
    "with open('User_Reddit_Classification_Results/rb_errors.json', 'w') as outfile:\n",
    "    json.dump(rb_errors, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299072ae-34ea-4af6-bac3-80a6d65fbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joeddav/distilbert-base-uncased-go-emotions-student\n",
    "from transformers import pipeline\n",
    "\n",
    "db_classifier = pipeline(task=\"text-classification\", model=\"joeddav/distilbert-base-uncased-go-emotions-student\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c39c17-58fc-4e89-a462-3bc36f78d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5797 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "db_outputs = []\n",
    "db_errors = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "  try:\n",
    "    db_output = db_classifier(sent)\n",
    "    db_outputs.append(db_output[0])\n",
    "  except:\n",
    "    db_errors.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce4bc9e-ae12-41d1-8631-7b58bafa056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the JSON object to a file\n",
    "with open('User_Reddit_Classification_Results/db_outputs.json', 'w') as outfile:\n",
    "    json.dump(db_outputs, outfile, indent=4)\n",
    "\n",
    "# Serialize the list to a JSON file\n",
    "with open('User_Reddit_Classification_Results/db_errors.json', 'w') as outfile:\n",
    "    json.dump(db_errors, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28c01741-b51c-43b4-a0cd-6203b6fb858b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5495, 28, 5491, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rb_outputs), len(rb_errors), len(db_outputs), len(db_errors), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f7250-ee74-4114-85ff-6215b8f01388",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8460a1-68db-4104-86e2-2091aa144efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "422684d5-0ebb-4558-9759-92f1202f9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a99e5e92-a55b-4f1c-af42-8078109d7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/msaxena4/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "# Define a function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "  stop_words = nltk.corpus.stopwords.words('english')\n",
    "  return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Define a function to lowercase the text\n",
    "def lowercase_text(text):\n",
    "  return text.lower()\n",
    "\n",
    "# Remove emojis & punctuation\n",
    "def replace_emojis(text):\n",
    "  emoji_pattern = r'[^\\w\\s]'\n",
    "  text = str(text)\n",
    "  return re.sub(emoji_pattern, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe49845e-657d-4c28-8d57-a294114df31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data['preprocessed_content1'] = reddit_data['preprocessed_content'].apply(replace_emojis).apply(remove_stop_words).apply(lowercase_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d359fa5-5868-4ebe-9bc9-b95592effd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_with_empty_text(df):\n",
    "\n",
    "    df = df.dropna(subset=['content'])\n",
    "    df = df[df['content'].str.len() >= 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7e250b2-359f-4c91-afb1-f3dc84ee58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = drop_rows_with_empty_text(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "379646b0-0619-4350-b055-d45fc40c5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.head()\n",
    "sentences = reddit_data['content'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91a88b92-9f0e-4c8d-83c6-de4c1483b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd0859a23c4d9ca1186c6010efaa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 15:49:48,327 - BERTopic - Transformed documents to Embeddings\n",
      "2023-11-13 15:49:53,250 - BERTopic - Reduced dimensionality\n",
      "2023-11-13 15:49:53,304 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "# topic_model = BERTopic.load(\"heyitskim1912/TopicModelling\")\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d958d95-f696-4d7c-83f7-15a2307f242d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>223</td>\n",
       "      <td>-1_the_to_and_it</td>\n",
       "      <td>[the, to, and, it, is, this, in, of, do, an]</td>\n",
       "      <td>[[LW] Ethical savescumming; a how-to.\\nI thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>0_the_and_it_to</td>\n",
       "      <td>[the, and, it, to, in, that, of, they, is, was]</td>\n",
       "      <td>[i found a game online similar to not pron tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>1_dollars_albums_coins_x200b</td>\n",
       "      <td>[dollars, albums, coins, x200b, type, dansco, ...</td>\n",
       "      <td>[[WTB] Odd, Foreign, Out of Print, or Custom D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>2_rick_astley_video_my</td>\n",
       "      <td>[rick, astley, video, my, pop, me, love, you, ...</td>\n",
       "      <td>[Rick Astley - Love This Christmas (Official V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>3_in_ernesto_she_who</td>\n",
       "      <td>[in, ernesto, she, who, liberals, dog, her, to...</td>\n",
       "      <td>[Retirement account beneficiary to children vs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                          Name  \\\n",
       "0     -1    223              -1_the_to_and_it   \n",
       "1      0    203               0_the_and_it_to   \n",
       "2      1    109  1_dollars_albums_coins_x200b   \n",
       "3      2     73        2_rick_astley_video_my   \n",
       "4      3     64          3_in_ernesto_she_who   \n",
       "\n",
       "                                      Representation  \\\n",
       "0       [the, to, and, it, is, this, in, of, do, an]   \n",
       "1    [the, and, it, to, in, that, of, they, is, was]   \n",
       "2  [dollars, albums, coins, x200b, type, dansco, ...   \n",
       "3  [rick, astley, video, my, pop, me, love, you, ...   \n",
       "4  [in, ernesto, she, who, liberals, dog, her, to...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [[LW] Ethical savescumming; a how-to.\\nI thoug...  \n",
       "1  [i found a game online similar to not pron tha...  \n",
       "2  [[WTB] Odd, Foreign, Out of Print, or Custom D...  \n",
       "3  [Rick Astley - Love This Christmas (Official V...  \n",
       "4  [Retirement account beneficiary to children vs...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = topic_model.get_topic_info(); freq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0368a3f2-74ef-4b46-b0a3-4fc465bd0891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.03780199106289492),\n",
       " ('and', 0.031439268293961),\n",
       " ('it', 0.03069503266552148),\n",
       " ('to', 0.02749363310180649),\n",
       " ('in', 0.026807278280429808),\n",
       " ('that', 0.025279121559162458),\n",
       " ('of', 0.023042890068609736),\n",
       " ('they', 0.02218742148748981),\n",
       " ('is', 0.020874491143482696),\n",
       " ('was', 0.020087955815640036)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(0)  # Select the most frequent topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1f252-08fc-460f-a746-cfe328e7c1a0",
   "metadata": {},
   "source": [
    "## transformers-interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "431071b4-5e6b-468e-91a8-669a8fd6286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f857c836-61a7-4418-8785-ed8f700cf27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
       "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
       "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
       "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
       "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
       "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goemotions1 = pd.read_csv(\"data/goemotions_1.csv\")\n",
    "goemotions1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1053e22c-e43f-4dd6-bf51-a3635fde58d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['admiration',\n",
    "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
    "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
    "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
    "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "       'remorse', 'sadness', 'surprise', 'neutral']\n",
    "len(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d1ce8f-81e2-4bd5-b933-11476aa30c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = reddit_data['preprocessed_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d25a11-6e5c-4147-be89-6054c2c4fd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 14:25:54.155892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-16 14:25:54.775772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc193ac5-5efa-4dfe-aae5-e5d89c930bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequenceclassification(model, tokenizer, sentence):\n",
    "    cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "    word_attributions = cls_explainer(sentence)\n",
    "    label = cls_explainer.predicted_class_name\n",
    "    return word_attributions, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce81a66-8713-4427-a537-4d7d84c47a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions_data = {\n",
    "    'word' : []\n",
    "}\n",
    "\n",
    "for emotion in emotions:\n",
    "    score_col = emotion + \"_score\"\n",
    "    count_count = emotion + \"_count\"\n",
    "    word_attributions_data[score_col] = []\n",
    "    word_attributions_data[count_count] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a871eed-5bf9-430c-aa78-c2a93955f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:   1%|▏         | 71/5523 [00:33<3:13:32,  2.13s/sentence]Token indices sequence length is longer than the specified maximum sequence length for this model (6636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing sentences: 100%|██████████| 5523/5523 [1:22:09<00:00,  1.12sentence/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rb_ti_errors = []     # roberta-base transformer interpret\n",
    "\n",
    "c = 0\n",
    "for sentence in tqdm(sentences, desc=\"Processing sentences\", unit=\"sentence\"):\n",
    "    c += 1\n",
    "    try:\n",
    "        word_attributions, label = sequenceclassification(model, tokenizer, sentence)\n",
    "        score_col = label + \"_score\"\n",
    "        count_col = label + \"_count\"\n",
    "        for item in word_attributions:\n",
    "            word = item[0]\n",
    "            score = item[1]\n",
    "            if item[0] not in word_attributions_data['word']:\n",
    "                word_attributions_data['word'].append(word)\n",
    "                idx = word_attributions_data['word'].index(word) # try this by len?\n",
    "                word_attributions_data[score_col].append(score)\n",
    "                word_attributions_data[count_col].append(1)\n",
    "                for emotion in emotions:\n",
    "                    if emotion != label:\n",
    "                        other_score_col = emotion + \"_score\"\n",
    "                        other_count_col = emotion + \"_count\"\n",
    "                        word_attributions_data[other_score_col].append(0.0)\n",
    "                        word_attributions_data[other_count_col].append(0)\n",
    "                          \n",
    "            else:\n",
    "                idx = word_attributions_data['word'].index(word)\n",
    "                word_attributions_data[score_col][idx] += score\n",
    "                word_attributions_data[count_col][idx] += 1\n",
    "            \n",
    "    except:\n",
    "        rb_ti_errors.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "231e3491-d90c-48bd-a8d4-afabd284bd8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>admiration_score</th>\n",
       "      <th>admiration_count</th>\n",
       "      <th>amusement_score</th>\n",
       "      <th>amusement_count</th>\n",
       "      <th>anger_score</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>annoyance_score</th>\n",
       "      <th>annoyance_count</th>\n",
       "      <th>approval_score</th>\n",
       "      <th>...</th>\n",
       "      <th>relief_score</th>\n",
       "      <th>relief_count</th>\n",
       "      <th>remorse_score</th>\n",
       "      <th>remorse_count</th>\n",
       "      <th>sadness_score</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_score</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>neutral_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.471796</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.019609</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.322971</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im</td>\n",
       "      <td>3.178083</td>\n",
       "      <td>51</td>\n",
       "      <td>1.657004</td>\n",
       "      <td>97</td>\n",
       "      <td>-0.002995</td>\n",
       "      <td>11</td>\n",
       "      <td>4.864147</td>\n",
       "      <td>69</td>\n",
       "      <td>0.624838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.775257</td>\n",
       "      <td>33</td>\n",
       "      <td>5.088185</td>\n",
       "      <td>231</td>\n",
       "      <td>0.931619</td>\n",
       "      <td>18</td>\n",
       "      <td>-28.446197</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r</td>\n",
       "      <td>-0.347111</td>\n",
       "      <td>21</td>\n",
       "      <td>0.080260</td>\n",
       "      <td>11</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266318</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177270</td>\n",
       "      <td>4</td>\n",
       "      <td>0.081016</td>\n",
       "      <td>8</td>\n",
       "      <td>0.090215</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.290583</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ick</td>\n",
       "      <td>-0.104215</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.787130</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13968</th>\n",
       "      <td>¿</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13969</th>\n",
       "      <td>ı</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13970</th>\n",
       "      <td>Ë</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13971</th>\n",
       "      <td>¢</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13972</th>\n",
       "      <td>heartfelt</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13973 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  admiration_score  admiration_count  amusement_score  \\\n",
       "0            <s>          0.000000               281         0.000000   \n",
       "1             hi          0.002015                 2         0.035689   \n",
       "2             im          3.178083                51         1.657004   \n",
       "3              r         -0.347111                21         0.080260   \n",
       "4            ick         -0.104215                 4         0.000000   \n",
       "...          ...               ...               ...              ...   \n",
       "13968          ¿          0.000000                 0         0.000000   \n",
       "13969          ı          0.000000                 0         0.000000   \n",
       "13970          Ë          0.000000                 0         0.000000   \n",
       "13971          ¢          0.000000                 0         0.000000   \n",
       "13972  heartfelt          0.000000                 0         0.000000   \n",
       "\n",
       "       amusement_count  anger_score  anger_count  annoyance_score  \\\n",
       "0                  113     0.000000           55         0.000000   \n",
       "1                    2     0.000000            0         0.000000   \n",
       "2                   97    -0.002995           11         4.864147   \n",
       "3                   11     0.008351            1         0.266318   \n",
       "4                    0     0.000000            0         0.000000   \n",
       "...                ...          ...          ...              ...   \n",
       "13968                0     0.000000            0         0.000000   \n",
       "13969                0     0.000000            0         0.000000   \n",
       "13970                0     0.000000            0         0.000000   \n",
       "13971                0     0.000000            0         0.000000   \n",
       "13972                0     0.000000            0         0.000000   \n",
       "\n",
       "       annoyance_count  approval_score  ...  relief_score  relief_count  \\\n",
       "0                   83        0.000000  ...           0.0             0   \n",
       "1                    0        0.000000  ...           0.0             0   \n",
       "2                   69        0.624838  ...           0.0             0   \n",
       "3                   10        0.000000  ...           0.0             0   \n",
       "4                    0        0.000000  ...           0.0             0   \n",
       "...                ...             ...  ...           ...           ...   \n",
       "13968                0        0.000000  ...           0.0             0   \n",
       "13969                0        0.000000  ...           0.0             0   \n",
       "13970                0        0.000000  ...           0.0             0   \n",
       "13971                0        0.000000  ...           0.0             0   \n",
       "13972                0        0.000000  ...           0.0             0   \n",
       "\n",
       "       remorse_score  remorse_count  sadness_score  sadness_count  \\\n",
       "0           0.000000             45       0.000000            155   \n",
       "1           0.471796              4      -0.019609              6   \n",
       "2           0.775257             33       5.088185            231   \n",
       "3           0.177270              4       0.081016              8   \n",
       "4           0.000000              0       0.005500              1   \n",
       "...              ...            ...            ...            ...   \n",
       "13968       0.000000              0       0.000000              0   \n",
       "13969       0.000000              0       0.000000              0   \n",
       "13970       0.000000              0       0.000000              0   \n",
       "13971       0.000000              0       0.000000              0   \n",
       "13972       0.000000              0       0.000000              0   \n",
       "\n",
       "       surprise_score  surprise_count  neutral_score  neutral_count  \n",
       "0            0.000000              37       0.000000           3405  \n",
       "1            0.000000               0       1.322971             46  \n",
       "2            0.931619              18     -28.446197            439  \n",
       "3            0.090215               2      -1.290583            317  \n",
       "4            0.000000               0      -0.787130             13  \n",
       "...               ...             ...            ...            ...  \n",
       "13968        0.000000               0       0.000000              0  \n",
       "13969        0.000000               0       0.000000              0  \n",
       "13970        0.000000               0       0.000000              0  \n",
       "13971        0.000000               0       0.000000              0  \n",
       "13972        0.000000               0       0.000000              0  \n",
       "\n",
       "[13973 rows x 57 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attributions_df = pd.DataFrame(word_attributions_data)\n",
    "# word_attributions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c502eead-387e-4d56-92d1-7b9a5171568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_attributions_df['admiration_score'].sort_values(ascending = False, inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c24af-9ed9-42ab-97ca-66e95c4e3b9e",
   "metadata": {},
   "source": [
    "1. Drop rows with 0 attribution values\n",
    "2. word cloud for each emotion - better way to analyse 28 word clouds?\n",
    "3. bar plot of each emotiom - num of words contibuting\n",
    "4. example words - plot\n",
    "5. make a tree-like structure for each emotion - venn diagram for top 10 words in each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea502e03-c949-4337-836d-5413249d5f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # Use CPU if no GPU is available\n",
    "print(\"using CPU\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4b700c2-77bd-461b-aba5-10d0267d1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path where you want to save the CSV file\n",
    "file_path = 'data/word_attributions_rb.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "word_attributions_df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "83e68ddc-134d-45a7-b883-8332ee74b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in top_10_words.items():\n",
    "#     print(k)\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8135ceba-5d9e-4911-ab12-2f60e892814b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
